# -*- coding: utf-8 -*-
"""toxic_comment_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oNiu5AjeNOjP_YUlpsHeU9KOVV3WYTlJ

# **Mounting Google Drive**
"""

from google.colab import drive
mount_path = "/content/drive"
drive.mount(mount_path)

# !ls "/content/drive/MyDrive/data/comment_data/train.csv"

"""# **Installing Dependencies and bringing data**

```
!pip install tensorflow tensorflow-gpu pandas matplotlib sklearn
```
"""

import os
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
import tensorflow as tf

df = pd.read_csv(
    os.path.join(mount_path,"MyDrive","data","comment_data","train.csv","train.csv")
)

df[df['toxic']==1].head()

df.tail()

df.iloc[6]['comment_text']

df[df.columns[2:]].iloc[6]

"""# **Preprocessing the data**"""

from tensorflow.keras.layers import TextVectorization

x = df['comment_text']
y = df[df.columns[2:]].values

y

MAX_FEATURES = 200000

vectorizer = TextVectorization(max_tokens = MAX_FEATURES, output_sequence_length = 1800, output_mode = 'int')

x.values

vectorizer.adapt(x.values)



vectorizer.get_vocabulary()

vectorizer("Hello World! life is hell")

vectorized_text = vectorizer(x.values)

vectorized_text

dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,y))
dataset = dataset.cache()
dataset = dataset.shuffle(160000)
dataset = dataset.batch(16)
dataset = dataset.prefetch(8)

batch_x, batch_y = dataset.as_numpy_iterator().next()

batch_x.shape

batch_y.shape

train = dataset.take(int(len(dataset)*0.7))
val = dataset.skip(int(len(dataset)*0.7)).take(int(len(dataset)*0.2))
test = dataset.skip(int(len(dataset)*0.9)).take(int(len(dataset)*0.1))

train.as_numpy_iterator().next()

"""# **Creating a sequential model**"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

model = Sequential()
# Creating the embedding layer
model.add(Embedding(MAX_FEATURES+1, 32))
# Creating LSTM layer
model.add(Bidirectional(LSTM(32, activation='tanh')))
model.add(Dropout(0.2))
# Feature extractor fully connected layers
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
# Final Layer
model.add(Dense(6, activation='sigmoid'))

# Compile model
model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])

# Set up early stopping
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# Train the model
history = model.fit(train, epochs=1, batch_size=16, validation_data=val, callbacks=[early_stop])

history.history

plt.figure(figsize=(8,5))
pd.DataFrame(history.history).plot()
plt.show()

"""# **Making Predictions**"""

batch_x, batch_y  = test.as_numpy_iterator().next()

text = vectorizer("I freaking hate you")

text

res = (model.predict(batch_x)>0.5).astype('int')

res

batch_y

"""# **Evaluating Model**"""

from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy

pre = Precision()
re = Recall()
acc = CategoricalAccuracy()

for batch in test.as_numpy_iterator():
  x_true, y_true = batch
  y_hat = model.predict(x_true)

  y_true = y_true.flatten()
  y_hat = y_hat.flatten()

  pre.update_state(y_true, y_hat)
  re.update_state(y_true, y_hat)
  acc.update_state(y_true, y_hat)

print(f'''Precision:{pre.result().numpy()},
          Recall:{re.result().numpy()},
          Accuracy:{acc.result().numpy()}''')

model.save('toxicy.h5')